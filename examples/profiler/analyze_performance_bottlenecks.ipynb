{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "training_job_name = 'profiler-gpu-2020-07-01-02-31-58-560'\n",
    "region = 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and analyze performance bottlenecks\n",
    "\n",
    "This notebook will detect performance bottlenecks and perform a simple root cause analysis for provided sagemaker training job.\n",
    "\n",
    "# 1. Install Dependencies\n",
    "\n",
    "\n",
    "###  `smdebug`\n",
    "\n",
    "The first thing you will need to do is install the private beta versions of the SageMaker Python SDK and the `smdebug` library. This will enable you to call a private version of the API that allows you to create SageMaker training jobs with the profiler enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! pip install -q ../smdebug-0.8.0b20200622-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read profiler data: system metrics and framework metrics\n",
    "\n",
    "Once the training job is running SageMaker will collect system and framework metrics. The following code cell is waiting for the system metrics & framework metrics to become available in S3. Once they are available you will be able to query and plot those metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "\n",
    "tj = TrainingJob(training_job_name, region)\n",
    "tj.wait_for_sys_profiling_data_to_be_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert data into Pandas frame\n",
    "\n",
    "## 2.1 Pandas frame for system metrics\n",
    " \n",
    "Smdebug provides a class `PandasFrame` that converts profiler data into Pandas frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.utils.profiler_data_to_pandas import PandasFrame\n",
    "\n",
    "pf = PandasFrame(tj.profiler_s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function retrieves the latest events. If your training job is still running, you can periodically run this function to update the Pandas frames with the latest profiling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_metrics_df, framework_metrics_df = pf.get_latest_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on the Pandas frame for system metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas frame for framework metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple Analysis\n",
    "\n",
    "## 3.1 Training job duration\n",
    "\n",
    "First we check how long the job has been running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training job run from:', min(system_metrics_df['timestamp']), 'to', max(system_metrics_df['timestamp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get timestamp when initialization ended and training loop started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_0 = framework_metrics_df[(framework_metrics_df['step'] == 0) \n",
    "                                           & (framework_metrics_df['framework_metric'] == 'Step:ModeKeys.TRAIN')]\n",
    "training_loop_start = step_0['start_time_us'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training loop started at ' + str(step_0['start_time']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last training/validation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop_end = max(framework_metrics_df[framework_metrics_df['framework_metric'].str.match('Step:ModeKeys.TRAIN')]['end_time_us'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training loop took  ' + str(training_loop_end - training_loop_start) + 'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pie chart for training phases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalization = max(system_metrics_df['timestamp_us']) - training_loop_end\n",
    "training_job_phases = {'initialization': training_loop_start,\n",
    "                       'training_loop': training_loop_end - training_loop_start, \n",
    "                       'finalization':  finalization if finalization > 0 else 0\n",
    "                      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(training_job_phases).plot.pie(figsize=(10, 10), \n",
    "                                     autopct='%.2f', \n",
    "                                     label='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Low GPU utilization\n",
    "\n",
    "Get all rows from Pandas frame which have GPU metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = system_metrics_df[system_metrics_df['system_metric'].str.match('gpu')]\n",
    "gpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all rows that have low GPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gpus.loc[gpus['value'] == 0.0]), \"datapoints out of\", len(gpus), \"where GPU utilization was at 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gpus.loc[gpus['value'] < 50.0]), \"datapoints out of\", len(gpus), \"where GPU utilization was below 50.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of GPU utilization values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus['value'].plot.hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get GPU utilization values during training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus_training_loop = gpus[(gpus['timestamp_us'] > training_loop_start) \n",
    "                          & (gpus['timestamp_us'] < training_loop_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus_training_loop['value'].plot.hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Low CPU utilization\n",
    "\n",
    "\n",
    "Get all rows from Pandas frame which have CPU metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = system_metrics_df[system_metrics_df['system_metric'].str.match('cpu')]\n",
    "cpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all rows that have low CPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cpus.loc[cpus['value'] == 0.0]), \"datapoints out of\", len(cpus), \"where CPU utilization was at 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cpus.loc[cpus['value'] < 50.0]), \"datapoints out of\", len(cpus), \"where CPU utilization was below 50.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of CPU utilization values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus['value'].plot.hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get CPU utilization during training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus_training_loop = cpus[(cpus['timestamp_us'] > training_loop_start) \n",
    "                          & (cpus['timestamp_us'] < training_loop_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus_training_loop['value'].plot.hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Detect CPU bottlenecks\n",
    "The time periods where GPU was at 0 percent but CPU was at a 100 percent may indicate a CPU bottleneck. Let's retrieve those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cpu = cpus[cpus['value'] > 90.0]\n",
    "low_gpu = gpus[gpus['value'] <= 10.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find rows where timestamp between both frames is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_bottleneck = low_gpu.merge(high_cpu, on='timestamp', how='inner')\n",
    "cpu_bottleneck = cpu_bottleneck.drop_duplicates(subset=['system_metric_x','timestamp'], keep='first')\n",
    "cpu_bottleneck.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestamp in cpu_bottleneck['timestamp'].unique():\n",
    "    print('CPU bottleneck observed at', timestamp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get CPU bottlenecks that happen during the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_bottleneck_training_loop = cpu_bottleneck[cpu_bottleneck['timestamp_us_x'] > training_loop_start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 GPU usage - overview\n",
    "\n",
    "Optimizing the performance of ML model training means to increase GPU usage. Let's create a Pie chart the shows the proportion between low and high GPU usage. We define the following intervals:\n",
    "- GPU usage above 90%: optimal GPU usage\n",
    "- GPU usage above 10% but below 90%: sub-optimal GPU usage\n",
    "- GPU usage below 10% but CPU usage not at 100%: low GPU usage \n",
    "- CPU usage above 90% and GPU usage below 10%: low GPU usage likely due to CPU bottleneck\n",
    "- initialization: GPU usage well be 0 because training loop has not started yet\n",
    "- finalization: the time from end of training loop to end of training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_usage = {'high_gpu_usage': len(gpus_training_loop[(gpus_training_loop['value'] > 90.0)]),\n",
    "                'suboptimal_gpu_usage': len(gpus_training_loop[(gpus_training_loop['value'] < 90.0) & (gpus_training_loop['value'] > 10.0)]),\n",
    "                'low_gpu_usage': len(gpus_training_loop[(gpus_training_loop['value'] <= 10.0)]) - len(cpu_bottleneck_training_loop),\n",
    "                'cpu_bottleneck': len(cpu_bottleneck_training_loop),\n",
    "                'initialization': len(gpus[gpus['timestamp_us'] < training_loop_start]),\n",
    "                'finalization': len(gpus[gpus['timestamp_us'] > training_loop_end])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.Series(system_usage).plot.pie(figsize=(7, 7), \n",
    "                                     autopct='%.2f', \n",
    "                                     label='')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Detect workload balancing issues in multi-GPU training\n",
    "\n",
    "If usage one one GPU is much higher compared to the remaining GPUs, then this indicates an isuee with how workload is distributed across multiple GPUs. The following code cell will compute a histogram of usage values per GPU and measure the distance between the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = len(gpus['system_metric'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute histogram of usage values per GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "distributions = {}\n",
    "for gpu in range(n_gpus):\n",
    "    values = gpus[gpus['system_metric'] == 'gpu' + str(gpu)]['value']\n",
    "    bins = np.arange(0, 100, 2)\n",
    "    probs, binedges = np.histogram(values, bins=bins)\n",
    "    distributions[gpu] = probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which usage patterns exceed the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "for gpu1 in range(n_gpus):\n",
    "    for gpu2 in range(gpu1+1, n_gpus):\n",
    "        m = (distributions[gpu1] + distributions[gpu2]) / 2\n",
    "        divergence = (scipy.stats.entropy(distributions[gpu1], m) + scipy.stats.entropy(distributions[gpu2], m)) / 2\n",
    "        distance = np.sqrt(divergence)\n",
    "        \n",
    "        if distance > 0.5:\n",
    "            print('Workload between GPU', gpu1, 'and', gpu2, 'differs significantly. ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Detect outliers in step duration\n",
    "\n",
    "We would expect that the average duration of training steps should be roughly the same. If there are significant outliers, it may indicate an issue. The following code cell computes the step duration and its median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = framework_metrics_df[framework_metrics_df['framework_metric']=='Step:ModeKeys.TRAIN']\n",
    "step_durations = train_steps['end_time_us'] - train_steps['start_time_us']\n",
    "median_train_steptime = step_durations.median()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step durations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_durations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median of training step durations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_train_steptime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we select all training steps where duration was larger than twice the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_train_steps = train_steps[step_durations > median_train_steptime * 2.0]\n",
    "outliers_train_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider all values below this threshold as normal step duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train_steps = train_steps[step_durations < median_train_steptime * 2.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Root cause analysis\n",
    "\n",
    "## 4.1. CPU bottlenecks\n",
    "In this section, we will retrieve the function calls that were made at the time that a CPU bottleneck occured. We then compare the average time spent in those functions with the average time spent when there is no bottleneck. The difference between both may give an indication what the possible root cause is. \n",
    "\n",
    "For instance if the dataloading time significantly increases when GPU is at 0 and CPU is at 100 percent versus the dataloading time when GPU is at 100 percent then we can conclude that there is an issue in the data processing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_bottleneck.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to compute cumulative time of a function for a certain timerange:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_time(unique_indexes):\n",
    "    cumulative_time = {}\n",
    "    training_phase = {}\n",
    "    for index in unique_indexes:\n",
    "        metric = framework_metrics_df['framework_metric'][index]\n",
    "        if 'Step:ModeKeys' in metric:\n",
    "            if metric not in training_phase:\n",
    "                training_phase[metric] = 0\n",
    "            training_phase[metric] += framework_metrics_df['end_time_us'][index] - framework_metrics_df['start_time_us'][index]\n",
    "        elif metric not in cumulative_time:\n",
    "            if metric not in cumulative_time:\n",
    "                cumulative_time[metric] = 0\n",
    "            cumulative_time[metric] += framework_metrics_df['end_time_us'][index] - framework_metrics_df['start_time_us'][index]\n",
    "    return cumulative_time, training_phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the timestamps when a CPU bottleneck occured and find framework metrics for this timestep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for timestamp in cpu_bottleneck['timestamp_us_x'].unique():\n",
    "    inds = framework_metrics_df[(framework_metrics_df['start_time_us']  <= timestamp) \n",
    "                                & (framework_metrics_df['end_time_us'] >= timestamp)].index.tolist()\n",
    "    if len(inds) > 0 :\n",
    "        indexes.extend(inds)\n",
    "unique_indexes = set(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have retrieved the indices of the relevant framework metrics, we sum metrics with the same name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_time, training_phase = get_cumulative_time(unique_indexes)\n",
    "cumulative_time, training_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.figure()\n",
    "pyplot.subplot(122)\n",
    "pyplot.subplots_adjust(wspace=1)\n",
    "pd.Series(cumulative_time).plot.pie(figsize=(12, 12), \n",
    "                                     autopct='%.2f', \n",
    "                                     label='')\n",
    "pyplot.subplot(121)\n",
    "pd.Series(training_phase).plot.pie(figsize=(12, 12), \n",
    "                                     autopct='%.2f', \n",
    "                                     label='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in cumulative_time:\n",
    "    print(\"Average time spent in function\", metric, \":\", cumulative_time[metric]/len(unique_indexes), \"us\")\n",
    "    \n",
    "for metric in training_phase:\n",
    "    print(\"Average time spent in function\", metric, \":\", training_phase[metric]/len(unique_indexes), \"us\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identified the functions that took most of the time when CPU bottlenecks occur. Let's compare the average time spent in those functions versus the average time spent in those functions when there is no bottleneck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_gpu = gpus[gpus['value'] > 90.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for timestamp in high_gpu['timestamp_us'].unique():\n",
    "    inds = framework_metrics_df[(framework_metrics_df['start_time_us']  <= timestamp) \n",
    "                                & (framework_metrics_df['end_time_us'] >= timestamp)].index.tolist()\n",
    "    if len(inds) > 0 :\n",
    "        indexes.extend(inds)\n",
    "unique_indexes = set(indexes)\n",
    "cumulative_time, training_phase = get_cumulative_time(unique_indexes)\n",
    "cumulative_time, training_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.figure()\n",
    "pyplot.subplot(122)\n",
    "pyplot.subplots_adjust(wspace=1)\n",
    "pd.Series(cumulative_time).plot.pie(figsize=(12, 12), \n",
    "                                     autopct='%.2f', \n",
    "                                     label='')\n",
    "pyplot.subplot(121)\n",
    "pd.Series(training_phase).plot.pie(figsize=(12, 12), \n",
    "                                     autopct='%.2f', \n",
    "                                     label='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in cumulative_time:\n",
    "    print(\"Average time spent in function\", metric, \":\", cumulative_time[metric]/len(unique_indexes), \"us\")\n",
    "\n",
    "for metric in training_phase:\n",
    "    print(\"Average time spent in function\", metric, \":\", training_phase[metric]/len(unique_indexes), \"us\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers in step duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_calls = framework_metrics_df[(framework_metrics_df[\"framework_metric\"] != 'Step:ModeKeys.TRAIN') \n",
    "                                      & (framework_metrics_df[\"framework_metric\"] != 'Step:ModeKeys.EVAL')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_calls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for start_time, end_time in zip(outliers_train_steps['start_time_us'].tolist(), \n",
    "                                outliers_train_steps['end_time_us'].tolist()):\n",
    "    inds = function_calls[(function_calls['end_time_us']  >= start_time) \n",
    "                        & (function_calls['start_time_us'] <= end_time)].index.tolist()\n",
    "    if len(inds) > 0 :\n",
    "        indexes.extend(inds)\n",
    "unique_indexes = set(indexes)\n",
    "cumulative_time, _ = get_cumulative_time(unique_indexes)\n",
    "cumulative_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cumulative_time).plot.pie(figsize=(6, 6), \n",
    "                                     autopct='%.2f', \n",
    "                                     label='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time = {}\n",
    "avg_time['cpu_bottleneck'] = {}\n",
    "for metric in cumulative_time:\n",
    "    avg_time['cpu_bottleneck'][metric] =  cumulative_time[metric]/len(unique_indexes)\n",
    "    print(\"Average time spent in function\", metric, \":\", cumulative_time[metric]/len(unique_indexes), \"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the average time spent in those functions versus the average time spent in those functions when train steps have a 'normal' duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for start_time, end_time in zip(normal_train_steps['start_time_us'].tolist(), \n",
    "                                normal_train_steps['end_time_us'].tolist()):\n",
    "    inds = function_calls[(function_calls['end_time_us']  >= start_time) \n",
    "                        & (function_calls['start_time_us'] <= end_time)].index.tolist()\n",
    "    if len(inds) > 0 :\n",
    "        indexes.extend(inds)\n",
    "unique_indexes = set(indexes)\n",
    "cumulative_time, _ = get_cumulative_time(unique_indexes)\n",
    "cumulative_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cumulative_time).plot.pie(figsize=(6, 6), \n",
    "                                     autopct='%.2f', \n",
    "                                     label='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time['normal'] = {}\n",
    "for metric in cumulative_time:\n",
    "    avg_time['normal'][metric] =  cumulative_time[metric]/len(unique_indexes)\n",
    "    print(\"Average time spent in function\", metric, \":\", cumulative_time[metric]/len(unique_indexes), \"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare average time spent in those functions when there is a CP bottlenck versus no bottleneck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(avg_time).plot(kind='bar', subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get CPU/GPU utilization during train step outliers. If utilization is low, it may indicate a system stall. This can for instance happen if a process is waiting for an I/O request being resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for start_time, end_time in zip(outliers_train_steps['start_time_us'].tolist(), \n",
    "                                outliers_train_steps['end_time_us'].tolist()):\n",
    "    inds = system_metrics_df[(system_metrics_df['timestamp_us']  >= start_time) \n",
    "                        & (system_metrics_df['timestamp_us'] <= end_time)].index.tolist()\n",
    "    if len(inds) > 0 :\n",
    "        indexes.extend(inds)\n",
    "unique_indexes = set(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_metrics_train_step_outliers = system_metrics_df.iloc[indexes,:]\n",
    "gpus = system_metrics_train_step_outliers[system_metrics_train_step_outliers['system_metric'].str.match('gpu')]\n",
    "cpus = system_metrics_train_step_outliers[system_metrics_train_step_outliers['system_metric'].str.match('cpu')]\n",
    "gpus['value'].plot.hist(bins=50, alpha=0.5, legend=True, label=\"GPUs\")\n",
    "cpus['value'].plot.hist(bins=50, alpha=0.5, legend=True, label=\"CPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison CPU/GPU utilization during when training steps have normal duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for start_time, end_time in zip(normal_train_steps['start_time_us'].tolist(), \n",
    "                                normal_train_steps['end_time_us'].tolist()):\n",
    "    inds = system_metrics_df[(system_metrics_df['timestamp_us']  >= start_time) \n",
    "                        & (system_metrics_df['timestamp_us'] <= end_time)].index.tolist()\n",
    "    if len(inds) > 0 :\n",
    "        indexes.extend(inds)\n",
    "unique_indexes = set(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_metrics_normal_train_step = system_metrics_df.iloc[indexes,:]\n",
    "gpus = system_metrics_normal_train_step[system_metrics_normal_train_step['system_metric'].str.match('gpu')]\n",
    "cpus = system_metrics_normal_train_step[system_metrics_normal_train_step['system_metric'].str.match('cpu')]\n",
    "gpus['value'].plot.hist(bins=50, alpha=0.5, legend=True, label=\"GPUs\")\n",
    "cpus['value'].plot.hist(bins=50, alpha=0.5, legend=True, label=\"CPUs\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
