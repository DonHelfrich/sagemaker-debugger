{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler Report\n",
    "This report summarizes the execution of the profiler built-in rules. Profiler runs a set of rules as the training is going on where each of them identifies certain performance issues. This notebook gives a description for each rule and details about the rule execution e.g. how often was the rule condition met \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown, Code, Image\n",
    "def pretty_print(df):\n",
    "    raw_html = df.to_html().replace(\"\\\\n\",\"<br>\").replace('<tr>','<tr style=\"text-align: left;\">')\n",
    "    return display(HTML(raw_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training job summary\n",
    "\n",
    "The following table gives a summary about the training job. The tables includes information about when the training job started and ended, how much time intialization, training loop and finalization took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def load_report(rule_name):\n",
    "    try:\n",
    "        report = json.load(open('/opt/ml/processing/outputs/profiler-reports/'+rule_name+'.json'))\n",
    "        if rule_name != 'MaxInitializationTime' and rule_name != 'OverallSystemUsage':\n",
    "            triggered = report['RuleTriggered']\n",
    "            datapoints = report['Datapoints']\n",
    "            display(Markdown(f\"\"\"The number of times the {rule_name} rule triggerd: {triggered}\"\"\"))\n",
    "            display(Markdown(f\"\"\"The number of events processed by {rule_name} rule: {datapoints}\"\"\"))\n",
    "        return report\n",
    "    except FileNotFoundError:\n",
    "        print (rule_name + ' not triggered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\n",
    "\n",
    "job_statistics = {}\n",
    "report = load_report('MaxInitializationTime')\n",
    "if report:\n",
    "    if \"first\" in report['Details'][\"step_num\"]:\n",
    "        first_step = report['Details'][\"step_num\"][\"first\"]\n",
    "        last_step = report['Details'][\"step_num\"][\"last\"]\n",
    "    job_statistics[\"start_time\"] = us_since_epoch_to_human_readable_time(report['Details']['job_start'] * 1000000)\n",
    "    job_statistics[\"end_time\"] = us_since_epoch_to_human_readable_time(report['Details']['job_end'] * 1000000)\n",
    "    job_statistics[\"job_duration_in_seconds\"] = (report['Details']['job_end'] - report['Details']['job_start']) \n",
    "    if \"first\" in report['Details'][\"step_num\"]:\n",
    "        job_statistics[\"training_loop_start\"] = us_since_epoch_to_human_readable_time(first_step)\n",
    "        job_statistics[\"training_loop_end\"] = us_since_epoch_to_human_readable_time(last_step)\n",
    "        job_statistics[\"training_loop_duration_in_seconds\"] = (last_step - first_step) / 1000000\n",
    "        job_statistics[\"initialization_in_seconds\"] = first_step/1000000 - report['Details']['job_start'] \n",
    "        job_statistics[\"finalization_in_seconds\"] = np.abs(report['Details']['job_end'] - last_step/1000000)\n",
    "        job_statistics[\"initialization_%\"] = job_statistics[\"initialization_in_seconds\"] / job_statistics[\"job_duration_in_seconds\"] * 100\n",
    "        job_statistics[\"training_loop_%\"] = job_statistics[\"training_loop_duration_in_seconds\"] / job_statistics[\"job_duration_in_seconds\"] * 100\n",
    "        job_statistics[\"finalization_%\"] = job_statistics[\"finalization_in_seconds\"] / job_statistics[\"job_duration_in_seconds\"] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "if job_statistics:\n",
    "    df = pd.DataFrame.from_dict(job_statistics, orient='index')\n",
    "    start_time = df[0]['start_time']\n",
    "    date = datetime.datetime.strptime(df[0]['start_time'], '%Y-%m-%dT%H:%M:%S:%f')\n",
    "    day = date.date().strftime(\"%m/%d/%Y\")\n",
    "    hour = date.time().strftime(\"%H:%M:%S\")\n",
    "    duration = int(df[0]['job_duration_in_seconds'])\n",
    "    display(Markdown(f\"\"\"Your training job started on **{day}** at **{hour}** and ran for **{duration}** seconds.\"\"\"))\n",
    "     \n",
    "    pretty_print(df)\n",
    "    if \"first\" in report['Details'][\"step_num\"]:\n",
    "        if job_statistics[\"finalization_%\"]  < 0:\n",
    "            job_statistics[\"finalization_%\"]  = 0\n",
    "        if job_statistics[\"training_loop_%\"] < 0:\n",
    "            job_statistics[\"training_loop_%\"] = 0\n",
    "        if job_statistics[\"initialization_%\"] < 0:\n",
    "            job_statistics[\"initialization_%\"] = 0\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.pie([job_statistics[\"initialization_%\"] , \n",
    "                job_statistics[\"training_loop_%\"] , \n",
    "                job_statistics[\"finalization_%\"] ], \n",
    "                autopct='%1.1f%%')\n",
    "        ax.legend(['initialization', 'training_loop', 'finalization'], bbox_to_anchor=(0.8, 0))\n",
    "        plt.show()\n",
    "    else:\n",
    "        display(Markdown(\"\"\"No step information available. Cannot calculate initialization and finalization time.\"\"\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System usage statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "report = load_report('OverallSystemUsage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if \"GPU\" in report[\"Details\"]:\n",
    "    for node_id in report[\"Details\"][\"GPU\"]:\n",
    "        gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\n",
    "        cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\n",
    "\n",
    "        if gpu_p95 < 70 and cpu_p95 < 70:\n",
    "            display(Markdown(f\"\"\"The 95th quantile of the total GPU utilization on node {node_id} is only **{int(gpu_p95)}%**. \n",
    "            The 95th quantile of the total CPU utilization is only **{int(cpu_p95)}%**. Node {node_id} is under-utilized. \n",
    "            You may want to consider switching to a smaller instance type.\"\"\"))\n",
    "        elif gpu_p95 < 70 and cpu_p95 > 70:\n",
    "            display(Markdown(f\"\"\"The 95th quantile of the total GPU utilization on node {node_id} is only **{int(gpu_p95)}%**. \n",
    "            However, the 95th quantile of the total CPU utilization is **{int(cpu_p95)}%**. GPUs on node {node_id} are under-utilized \n",
    "            likely because of CPU bottlenecks\"\"\"))\n",
    "        elif gpu_p95 > 70:\n",
    "            display(Markdown(f\"\"\"The 95th quantile of the total GPU utilization on node {node_id} is **{int(gpu_p95)}%**. \n",
    "            GPUs on node {node_id} are well utilized\"\"\"))\n",
    "        else:\n",
    "            display(Markdown(f\"\"\"The 95th quantile of the total GPU utilization on node {node_id} is **{int(gpu_p95)}%**. \n",
    "            The 95th quantile of the total CPU utilization is {int(cpu_p95)}%.\"\"\"))\n",
    "else:\n",
    "    for node_id in report[\"Details\"][\"CPU\"]:\n",
    "        cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\n",
    "        if cpu_p95 > 70:\n",
    "            display(Markdown(f\"\"\"The 95th quantile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%**. GPUs on node {node_id} are well utilized\"\"\"))\n",
    "\n",
    "display(Markdown(f\"\"\"The following table shows usage statistics per worker node such as total CPU and GPU \n",
    "utilization, total CPU and memory footprint. The table also include total IO wait time and total sent/received bytes.\n",
    "The table shows min and max values as well as p99, p90 and p50 percentiles.\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "rows = [] \n",
    "units = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\n",
    "if report:\n",
    "    for metric in report['Details']:\n",
    "        for node_id in report['Details'][metric]:\n",
    "            values = report['Details'][metric][node_id]\n",
    "            rows.append([node_id, metric, units[metric], values['max'], values['p99'], values['p95'], values['p50'], values['min']])\n",
    "\n",
    "    df = pd.DataFrame(rows) \n",
    "    df.columns = ['Node', 'metric', 'unit', 'max', 'p99', 'p95', 'p50', 'min']\n",
    "    with pd.option_context('display.colheader_justify','left'):\n",
    "        pretty_print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "description = {}\n",
    "description['CPUBottleneck'] = 'Checks if CPU usage is high but GPU usage is low at the same time, it may indicate a CPU bottleneck where GPU is waiting for data to arrive from CPU. The rule triggers if number of CPU bottlenecks exceeds a predefined threshold.'\n",
    "description['IOBottleneck'] = 'If IO wait time is high but at the same time GPU usage is low, it may indicate an IO bottleneck where GPU is waiting for data to arrive from disk. The rule triggers if number of IO bottlenecks exceeds a predefined threshold.'\n",
    "#description['Dataloaders'] = 'Checks how many dataloader processes are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it may lead to low GPU utilization. If too large it may lead to too many context switches on CPU.'\n",
    "description['GPUMemoryIncrease'] = 'If model and/or batch size is too large then training will run out of memory and crash.'\n",
    "description['BatchSize'] = 'Checks if GPU is underulitized because of the batch size being too small. To detect this the rule analyzes the average GPU memory footprint, CPU and GPU utilization. '\n",
    "description['LowGPUUtilization'] = 'Checks if GPU utilization is low or suffers from fluctuations. This can happen if there are bottlenecks, many blocking calls due to synchroniziations or batch size too small.'\n",
    "description['MaxInitializationTime'] = 'Checks if the training intialization is taking too much time. The rule waits until first step is available. This can happen if you are running in File mode and a lot of data needs to be downloaded from Amazon S3.'\n",
    "description['LoadBalancing'] = 'Detect issues in workload balancing between multiple GPUs. Workload imbalance can for instance occur in data parallel training when gradients are accumulated on primary GPU so this GPU will be overused with regards to other GPUs limiting the effect of parallelization.  '\n",
    "description['StepOutlier'] = 'Detect outliers in step duration. Time for forward and backward pass should be roughly the same throughout the training. If there are significant outliers it would indicate an issue due to a system stall or a bottleneck.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "recommendation = {}\n",
    "recommendation['CPUBottleneck'] = 'CPU bottlenecks can happen when data preprocessing is very compute intensive. You should consider increasing the number of dataloaders or apply prefetching.'\n",
    "recommendation['IOBottleneck'] = 'Prefetch data or choose different file formats such as binary formats which improves read performance.'\n",
    "#recommendation['Dataloaders'] = 'Increase or decrease the number of dataloader subprocesses'\n",
    "recommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory (if it is not a memory leak) or apply model parallelism (Rubik)'\n",
    "recommendation['BatchSize'] = 'Run on a smaller instance type or increase batch size'\n",
    "recommendation['LowGPUUtilization'] = 'Check for bottlenecks, minimize blocking calls, change distributed training strategy, increase batchsize.'\n",
    "recommendation['MaxInitializationTime'] = 'Switch from File to Pipe mode'\n",
    "recommendation['LoadBalancing'] = 'Choose different distributed training strategy or different distributed training framework'\n",
    "recommendation['StepOutlier'] = 'Check for bottlenecks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "files = glob.glob('/opt/ml/processing/outputs/profiler-reports/*json')\n",
    "summary = {}\n",
    "for i in files:\n",
    "    rule_name = i.split('/')[-1].replace('.json','')\n",
    "    if rule_name == \"OverallSystemUsage\":\n",
    "        continue\n",
    "    rule_report = json.load(open(i))\n",
    "    summary[rule_name] = {}\n",
    "    summary[rule_name]['Description'] = description[rule_name]\n",
    "    summary[rule_name]['Recommendation'] = recommendation[rule_name]\n",
    "    summary[rule_name]['Number of times rule triggered'] = rule_report['RuleTriggered'] \n",
    "    summary[rule_name]['Number of violations'] = rule_report['Violations'] \n",
    "    summary[rule_name]['Number of datapoints'] = rule_report['Datapoints']\n",
    "    summary[rule_name]['Rule parameters'] = rule_report['RuleParameters']\n",
    "\n",
    "df = pd.DataFrame.from_dict(summary, orient='index')\n",
    "df = df.sort_values(by=['Number of times rule triggered'], ascending=False)\n",
    "\n",
    "display(Markdown(f\"\"\"The following table shows a summary of the executed profiler rules. \n",
    "The table is sorted by the rules the triggered most frequently. In your training job this was the case\n",
    "for rule **{df.index[0]}**.\n",
    "Per default rules will run on a time segment of 60 seconds. Within this time segment\n",
    "a rule may produce multiple violations e.g. a CPUBottleneck triggers once per time segment, but \n",
    "within that time segment there could be hundreds of bottlenecks. \n",
    "\n",
    "In your training job rule **{df.index[0]}** triggered **{df.values[0,2]}** times and recorded **{df.values[0,3]}** violations \"\"\"))\n",
    "with pd.option_context('display.colheader_justify','left'):    \n",
    "    pretty_print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "analyse_phase = \"training\"\n",
    "if job_statistics and \"initialization_in_seconds\" in job_statistics:\n",
    "    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\n",
    "        analyse_phase = \"initialization\"\n",
    "        display(Markdown(\"Since initialization has taken the most time, we dive deep into the events occurring during this phase\"))\n",
    "        display(Markdown(\"\"\"### Analysing initialization\\n\\n\"\"\"))\n",
    "if analyse_phase == \"training\":\n",
    "    display(Markdown(\"Since training loop has taken the most time, we dive deep into the events occurring during this phase\"))\n",
    "    display(Markdown(\"\"\"### Analysing the training loop\\n\\n\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def display_image(image_name):\n",
    "    files = glob.glob('/opt/ml/processing/outputs/profiler-reports/' + image_name)\n",
    "    for filename in files:\n",
    "        display(Image(filename=filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if analyse_phase == \"initialization\":\n",
    "    display(Markdown(\"\"\"### MaxInitializationTime\\n\\nThis rule helps to detect if the training intialization is taking too much time. \\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\nYou can run the rule locally in the following way:\n",
    "    \"\"\"))\n",
    "    display(Code('''\n",
    "    from smdebug.profiler.analysis.rules.max_intialization_time import MaxInitializationTime\n",
    "\n",
    "    profiler_path = estimator.latest_job_profiler_artifacts_path()\n",
    "    trial = create_trial(profiler_path, profiler=True)\n",
    "    rule = MaxInitializationTime(trial, threshold=20)\n",
    "    \n",
    "    def run_rule(rule):\n",
    "        try:\n",
    "            invoke_rule(rule, raise_eval_cond=True)\n",
    "        except NoMoreData:\n",
    "            print(\n",
    "                \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "            )\n",
    "        except RuleEvaluationConditionMet as e:\n",
    "            print(e)\n",
    "    \n",
    "    run_rule(rule)\n",
    "\n",
    "    ''', language=\"python\"))\n",
    "    \n",
    "    _ = load_report(\"MaxInitializationTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if analyse_phase == \"training\":\n",
    "    display(Markdown(\"\"\"#### Step duration analysis\"\"\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"The StepOutlier rule measures step durations and checks for outliers.\\nThe rule \\\n",
    "    returns `True` if duration is larger than `stddev` multiplied the standard deviation. The rule \\\n",
    "    also takes the parameter `mode`, that specifies whether steps from training or validation phase \\\n",
    "    should be checked. Typically the first step is taking signifciantly more time and to avoid the \\\n",
    "    rule triggering immedietaly, one can use `n_outliers` to specify the number of outliers to ignore.\\nYou \\\n",
    "    can run the rule locally in the following way:\n",
    "    \"\"\"))\n",
    "    \n",
    "    display(Code('''\n",
    "    from smdebug.profiler.analysis.rules.step_outlier import StepOutlier\n",
    "    \n",
    "    profiler_path = estimator.latest_job_profiler_artifacts_path()\n",
    "    trial = create_trial(profiler_path, profiler=True)\n",
    "    rule = StepOutlier(trial, stddev=5, mode=None, n_outliers=10, )\n",
    "\n",
    "    run_rule(rule)\n",
    "    ''', language=\"python\"))\n",
    "\n",
    "    report = load_report('StepOutlier')\n",
    "\n",
    "    if report and len(report['Details']) > 0:\n",
    "        for node_id in report['Details']:\n",
    "            tmp = report['RuleParameters'].split('threshold:')\n",
    "            threshold = tmp[1].split('\\n')[0]\n",
    "            n_outliers = report['Details'][node_id]['number_of_outliers']\n",
    "            mean = report['Details'][node_id]['step_stats']['mean']\n",
    "            stddev = report['Details'][node_id]['stddev']\n",
    "            phase = report['Details'][node_id]['phase']\n",
    "            display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node **{node_id}**.\n",
    "            The rule has analyzed the step duration from **{phase}** phase.\n",
    "            The average step duration on node **{node_id}** was **{round(mean, 3)}s**. \n",
    "            The rule detected **{n_outliers} outliers**, where step duration was larger than **{threshold} times** the standard deviation of **{stddev}s**\n",
    "                             \\n\"\"\"))\n",
    "            step_stats_df = pd.DataFrame.from_dict(report['Details'][node_id]['step_stats'], orient='index').T\n",
    "            step_stats_df.index = ['Step Durations in [s]']\n",
    "            pretty_print(step_stats_df)\n",
    "            if report['RuleTriggered'] > 0:\n",
    "                display(Markdown(f'\\nThe rule found the following outliers on node **{node_id}**'))\n",
    "                if 'outliers' in report['Details'][node_id]:\n",
    "                    for duration, step in zip(report['Details'][node_id]['outliers'][0], report['Details'][node_id]['step_numbers']):\n",
    "                        display(Markdown(f\"\"\"Step {step}: {duration} s\"\"\"))\n",
    "                        \n",
    "                display(Markdown(f\"\"\"To get a better uderstanding of what may have caused those outliers,\n",
    "                we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\n",
    "                The left chart shows how much time was spent in the different framework\n",
    "                metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\n",
    "                outliers). \"\"\"))\n",
    "                display_image(\"*step_duration_histogram.png\")\n",
    "\n",
    "\n",
    "                display(Markdown(\"\"\"The following chart shows how much time was spent in the different \n",
    "                framework metrics when step outliers occured. In this chart framework metrics are not aggregated by event phase.\"\"\"))\n",
    "                display_image(\"histogram_step_outlier_framework.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if analyse_phase == \"training\":\n",
    "    display(Markdown(\"\"\"#### GPU utilization analysis\\n\\n\"\"\"))\n",
    "\n",
    "    display(Markdown(\"\"\"The LowGPUUtilization rule checks for low and fluctuating GPU usage.\\nIf usage is consistently low \\\n",
    "    it can be caused by bottlenecks or if batch size is too small.\\nIf usage is heavily \\\n",
    "    fluctuating it can be caused by bottlenecks or blocking calls. User can specify a \\\n",
    "    threshold `threshold_p95` for 95th quantile and `threshold_p5` for 5th quantile.\\\n",
    "    Default values are 70% and 10%. If p95 is high and p5 is low it would indicate that the \\\n",
    "    usage is highly fluctuating. If both values are very low it would mean that the machine \\\n",
    "    is underutilized. Furthermore user can specify the window size on which the quantiles \\\n",
    "    will be computed. Default value is 500. During intilization GPU usage will be likely 0, \\\n",
    "    so the rule also takes a parameter `patience` that defines how many number of datapoints \\\n",
    "    to skip.\\nYou can run the rule locally in the following way:\n",
    "    \"\"\"))\n",
    "    display(Code('''\n",
    "    from smdebug.profiler.analysis.rules.low_gpu_utilization import LowGPUUtilization\n",
    "\n",
    "    profiler_path = estimator.latest_job_profiler_artifacts_path()\n",
    "    trial = create_trial(profiler_path, profiler=True)\n",
    "    rule = LowGPUUtilization(trial, threshold_p95=70, threshold_p95=10, patience=100, window=500)\n",
    "\n",
    "    run_rule(rule)  \n",
    "    ''', language=\"python\"))\n",
    "    report = load_report('LowGPUUtilization')\n",
    "    tmp = report['RuleParameters'].split(':')\n",
    "    threshold_p95 = tmp[1].split('\\n')[0]\n",
    "    threshold_p5 = tmp[2].split('\\n')[0]\n",
    "    window = tmp[3].split('\\n')[0]\n",
    "    display(Markdown(f\"\"\" The rule ran with parameters  **threshold_p95={threshold_p95}** and \n",
    "    **threshold_p5={threshold_p5}**. Those quantiles were computed on a window size of **{window}** \n",
    "    continous datapoints.\n",
    "     \"\"\"))\n",
    "    for node_id in report['Details']:\n",
    "        for gpu_id in report['Details'][node_id]:\n",
    "            p_95 = report['Details'][node_id][gpu_id]['gpu_95']\n",
    "            p_5 = report['Details'][node_id][gpu_id]['gpu_5']\n",
    "            if p_95 < int(threshold_p95): \n",
    "                display(Markdown(f\"\"\"The rule discovered that on node **{node_id}** the 95th percentile on **{gpu_id}** is only **{p_95}%**. \n",
    "                **{gpu_id}** on node **{node_id}** is under-utilized\"\"\"))\n",
    "            if p_5 < int(threshold_p5): \n",
    "                display(Markdown(f\"\"\"The rule discovered that the 5th percentile on **{gpu_id}** is only **{p_5}%**\"\"\"))\n",
    "            if p_95 - p_5 > 50:\n",
    "                display(Markdown(f\"\"\"The difference between 5th quantile **{p_5}%** and 95th quantile **{p_95}%** is quite \n",
    "                signficant, which means that utilization on **{gpu_id}** is fluctuating quite a lot.\"\"\"))\n",
    "    if len(report['Details']) > 0:\n",
    "        display(Markdown(f\"\"\"The following chart shows the boxplots of utilizations for the different GPUs.\"\"\"))\n",
    "    display_image(\"*box_plot_gpu_utilization.png\")\n",
    "    \n",
    "    if report:\n",
    "        display(Markdown(\"\"\"Since the LowGPUUtilization rule pointed out potential GPU under-utilization, we will make use of \\\n",
    "        two other rules BatchSize and CPUBottleneck to identify the cause for the low\n",
    "        utilization.\n",
    "        \"\"\"))\n",
    "        \n",
    "        display(Markdown(\"\"\"The BatchSize rule helps to detect if GPU is underulitized because of the batch size being \n",
    "        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU \n",
    "        utilization. If 95th quantile of CPU utilization is below `cpu_threshold_p95`, \n",
    "        95th quantile of GPU utilization is below `gpu_threshold_p95` and memory footprint \n",
    "        below `gpu_memory_threshold_p95` , it may indicate that user can either run on a \n",
    "        smaller instance type or that batch size could be increased. This analysis does \n",
    "        not work for frameworks that heavily over-allocate memory. Increasing batch size \n",
    "        could potentially lead to a processing/dataloading bottleneck, because more data \n",
    "        needs to be pre-processed in each iteration.\\nThe rule takes in addition a parameter \n",
    "        `patience` to skip the first few datapoints and a parameter `window` that defines \n",
    "        the number of datapoints over which the quantiles are computed.\\nYou can run the rule locally in the following way:\n",
    "        \"\"\"))\n",
    "        \n",
    "        display(Code('''\n",
    "        from smdebug.profiler.analysis.rules.batch_size import BatchSize\n",
    "\n",
    "        profiler_path = estimator.latest_job_profiler_artifacts_path()\n",
    "        trial = create_trial(profiler_path, profiler=True)\n",
    "        rule = BatchSize(trial, cpu_threshold_p95=70, gpu_threshold_p95=70, gpu_memory_threshold_p95=70, patience=100,  \n",
    "                 window=500)\n",
    "\n",
    "        run_rule(rule)\n",
    "        ''', language=\"python\"))\n",
    "        report = load_report('BatchSize')\n",
    "        tmp = report['RuleParameters'].split(':')\n",
    "        cpu_threshold_p95 = int(tmp[1].split('\\n')[0])\n",
    "        gpu_threshold_p95 = int(tmp[2].split(' ')[0])\n",
    "        gpu_memory_threshold_p95 = int(tmp[3].split('\\n')[0])\n",
    "        display(Markdown(f\"\"\" The rule ran with parameters  **cpu_threshold_p95={cpu_threshold_p95}**, **gpu_threshold_p95={gpu_threshold_p95}** and **gpu_memory_threshold_p95={gpu_memory_threshold_p95}**\n",
    "         \"\"\"))\n",
    "        for node_id in report['Details']:\n",
    "            display(Markdown(f\"\"\"Total CPU utilization p95 on node **{node_id}** is only **{cpu_p95}%**\"\"\"))\n",
    "            for gpu_id in report['Details'][node_id]:\n",
    "                cpu_p95 = round(report['Details'][node_id][gpu_id]['cpu_p95'], 2)\n",
    "                gpu_p95 = round(report['Details'][node_id][gpu_id]['gpu_p95'], 2)\n",
    "                gpu_memory_p95 = round(report['Details'][node_id][gpu_id]['gpu_memory_p95'], 2)\n",
    "                display(Markdown(f\"\"\"The 95th quantile of GPU utilization and memory utilization on **{gpu_id}** \n",
    "                is only  **{gpu_p95}%** and **{gpu_memory_p95}%**. \n",
    "                \"\"\"))\n",
    "        if len(report['Details']) >0:   \n",
    "            display(Markdown(f\"\"\"Your training job is under-utilizing the instance. You may want to consider\n",
    "            to either switch to a smaller instance type or to increase batch size of your model training. The following boxplots show the \n",
    "            total CPU usage and the usage and memory per GPU.\"\"\"))\n",
    "            \n",
    "        display_image('*box_plot_batch_size.png')\n",
    "        \n",
    "        \n",
    "        display(Markdown(\"\"\"The CPUBottleneck rule identifies when CPU utilization is high (above `cpu_threshold` default \n",
    "        is 90%) and GPU utilization is low (below `gpu_threshold` default is 10%).\\\n",
    "        GPU utilization is likely 0 during intiialization before the training loop has started,\n",
    "        so the rule takes an additional argument `patience` that defines how many datapoints \n",
    "        to capture before to run the first evaluation. The parameter `threshold` defines when \n",
    "        the rule should return True and default is 50. If we see CPU bottlencks 50% of the \n",
    "        time, then the rule will trigger.\\nYou can run the rule locally in the following way:\n",
    "        \"\"\"))\n",
    "\n",
    "        display(Code('''\n",
    "        from smdebug.profiler.analysis.rules.cpu_bottleneck import CPUBottleneck\n",
    "\n",
    "        profiler_path = estimator.latest_job_profiler_artifacts_path()\n",
    "        trial = create_trial(profiler_path, profiler=True)\n",
    "        rule = CPUBottleneck(trial, gpu_threshold=10, cpu_threshold=90, threshold=50, patience=100)\n",
    "\n",
    "        run_rule(rule)\n",
    "        ''', language=\"python\"))\n",
    "\n",
    "        report = load_report('CPUBottleneck')\n",
    "        if report:\n",
    "            violations = report['Violations']\n",
    "            perc = int(report['Violations']/report['Datapoints']*100)\n",
    "            tmp = report['RuleParameters'].split(':')\n",
    "            cpu_threshold = tmp[2].split('\\n')[0]\n",
    "            gpu_threshold = tmp[3].split('\\n')[0]\n",
    "            low_gpu = report['Details']['low_gpu_utilization']\n",
    "            datapoints = report['Datapoints']\n",
    "            n_bottlenecks = round(len(report['Details'])/datapoints * 100, 2)\n",
    "            display(Markdown(f\"\"\"The rule ran with the **cpu_threshold={cpu_threshold}%** and \n",
    "            **gpu_threshold={gpu_threshold}%**. With this configuration the rule found **{violations}** CPU bottlenecks\n",
    "            which is **{perc}%** of the total time.\"\"\"))\n",
    "            if report['RuleTriggered'] > 0:\n",
    "\n",
    "                display(Markdown(f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of **{gpu_threshold}%** \n",
    "                and how many of those datapoints were likely caused by a CPU bottleneck. \n",
    "                The rule found **{low_gpu}** out of **{datapoints}** datapoints which had a GPU utilization \n",
    "                below **{gpu_threshold}%**. \n",
    "                Out of those datapoints **{n_bottlenecks}%** were likely caused by CPU bottlenecks.\n",
    "                The chart in the middle shows how much time was spent in the framework metrics (aggregated by event phase) when CPU bottleneck occured. The chart on the right shows whether CPU bottlenecks mainly occured during training or validation phase.\n",
    "                \"\"\"))\n",
    "                display_image('pie_charts_cpu_bottleneck.png')\n",
    "\n",
    "                display(Markdown(\"\"\"The following chart shows how much time was spent in the different framework metrics while the CPU bottlenecks occured.\n",
    "                \"\"\"))\n",
    "                display_image('histogram_cpu_bottleneck_framework.png')\n",
    "        \n",
    "        display(Markdown(\"\"\"The IOBottleneck rule identifies when I/O wait time is above `io_threshold` (default \n",
    "        is 90%) and GPU utilization is low (below `gpu_threshold` default is 10%).\n",
    "        GPU utilization is likely 0 during intiialization before the training loop has started,\n",
    "        so the rule takes an additional argument `patience` that defines how many datapoints \n",
    "        to capture before to run the first evaluation. The parameter `threshold` defines when \n",
    "        the rule should return True and default is 50. If we see IO bottlencks 50% of the \n",
    "        time, then the rule will trigger.\\nYou can run the rule locally in the following way:\n",
    "        \"\"\"))\n",
    "\n",
    "        display(Code('''\n",
    "        from smdebug.profiler.analysis.rules.cpu_bottleneck import IOBottleneck\n",
    "\n",
    "        profiler_path = estimator.latest_job_profiler_artifacts_path()\n",
    "        trial = create_trial(profiler_path, profiler=True)\n",
    "        rule = IOBottleneck(trial, gpu_threshold=10, io_threshold=90, threshold=50, patience=100)\n",
    "\n",
    "        run_rule(rule)\n",
    "        ''', language=\"python\"))\n",
    "\n",
    "        report = load_report('IOBottleneck')\n",
    "        violations = report['Violations']\n",
    "        perc = int(report['Violations']/report['Datapoints']*100)\n",
    "        tmp = report['RuleParameters'].split(':')\n",
    "        io_threshold = tmp[2].split('\\n')[0]\n",
    "        gpu_threshold = tmp[3].split('\\n')[0]\n",
    "        low_gpu = report['Details']['low_gpu_utilization']\n",
    "        datapoints = report['Datapoints']\n",
    "        n_bottlenecks = round(len(report['Details'])/datapoints *100, 2)\n",
    "        display(Markdown(f\"\"\"The rule ran with the **io_threshold={io_threshold}%** and \n",
    "        **gpu_threshold={gpu_threshold}%**. With this configuration the rule found **{violations}** I/O bottlenecks\n",
    "        which is **{perc}%** of the total time.\"\"\"))\n",
    "        if report['RuleTriggered'] > 0:\n",
    "\n",
    "            display(Markdown(f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of **{gpu_threshold}%** \n",
    "            and how many of those datapoints were likely caused by an I/O bottleneck. \n",
    "            The rule found **{low_gpu}** out of **{datapoints}** datapoints which had a GPU utilization below **{gpu_threshold}%**. \n",
    "            Out of those datapoints **{n_bottlenecks}%** were likely caused by I/O bottlenecks.\n",
    "            The chart in the middle shows how much time was spent in the framework metrics (aggregated by event phase) when I/O bottleneck occured. \n",
    "            The chart on the right shows whether I/O bottlenecks mainly occured during training or validation phase.\n",
    "            \"\"\"))\n",
    "            display_image('pie_charts_io_bottleneck.png')\n",
    "\n",
    "            display(Markdown(\"\"\"The following chart shows how much time was spent in the different framework metrics when I/O bottlenecks occured.\n",
    "            \"\"\"))\n",
    "            display_image('histogram_io_bottleneck_framework.png')\n",
    "            \n",
    "    display(Markdown(\"\"\"The LoadBalancing rule helps to detect issues in workload balancing between multiple GPUs. \n",
    "    It computes a histogram of GPU utilization values for each GPU and compares then the \n",
    "    similarity between histograms. The rule takes a parameter `threshold` that defines \n",
    "    the maximum distance between histograms. In the beginning utilization is likely 0 when \n",
    "    intilization is happening. The parameter `patience` defines how many datapoints to skip \n",
    "    in the beginning.\\nYou can run the rule locally in the following way:\n",
    "    \"\"\"))\n",
    "    \n",
    "    display(Code('''\n",
    "    from smdebug.profiler.analysis.rules.load_balancing import LoadBalancing\n",
    "\n",
    "    profiler_path = estimator.latest_job_profiler_artifacts_path()\n",
    "    trial = create_trial(profiler_path, profiler=True)\n",
    "    rule = LoadBalancing(trial, threshold=0.5, patience=10)\n",
    "\n",
    "    run_rule(rule)\n",
    "    ''', language=\"python\"))\n",
    "    \n",
    "    report = load_report('LoadBalancing')\n",
    "    display_image('*load_balancing_workload.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if analyse_phase == \"training\":\n",
    "    display(Markdown(\"\"\"#### GPU memory analysis\\n\\n\"\"\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \n",
    "    The rule takes the parameter `increase` which defines the threshold for absolute \n",
    "    memory increase and the default is 10%. So if the moving average increases from 10% to 21%, \n",
    "    the rule will trigger. The parameter `patience` specifies how many datapoints to \n",
    "    capture before Rule runs the first evluation. Default 100. The parameter `window` \n",
    "    defines the window size for moving average.\\nYou can run the rule locally in the following way:\n",
    "    \"\"\"))\n",
    "    \n",
    "    display(Code('''\n",
    "    from smdebug.profiler.analysis.rules.gpu_usage import GPUMemoryIncrease\n",
    "\n",
    "    profiler_path = estimator.latest_job_profiler_artifacts_path()\n",
    "    trial = create_trial(profiler_path, profiler=True)\n",
    "    rule = GPUMemoryIncrease(trial, increase=10, patience=100, window=10)\n",
    "\n",
    "    run_rule(rule)\n",
    "    ''', language=\"python\"))\n",
    "    _ = load_report('GPUMemoryIncrease')\n",
    "    display_image('*box_plot_gpu_memory.png')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
